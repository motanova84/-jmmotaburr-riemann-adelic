{
 "cells": [
  {
   "cell_type": "markdown", 
   "metadata": {},
   "source": [
    "# Riemann Hypothesis Numerical Validation\n",
    "\n",
    "ðŸ§  **Copilot Prompt:**\n",
    "\n", 
    "Este cuaderno valida numÃ©ricamente la fÃ³rmula explÃ­cita propuesta en el artÃ­culo principal.\n",
    "Por favor, sugiere:\n",
    "\n",
    "- ComparaciÃ³n dinÃ¡mica entre las versiones de A_infty y zero_sum.\n",
    "- GrÃ¡ficos de convergencia de cada suma.\n",
    "- Tabla resumen con errores relativos y absolutos.\n",
    "\n",
    "Ejecuta automÃ¡ticamente con nbconvert y exporta a docs/validation.html.\n",
    "\n",
    "## Dynamic Comparison Framework\n",
    "\n",
    "This notebook provides dynamic validation of different versions of the Riemann zeta function explicit formula validation, with interactive convergence analysis and comprehensive error reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpmath as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "\n",
    "# Set high precision\n",
    "mp.mp.dps = 50\n",
    "\n",
    "print(f\"ðŸ”§ mpmath precision: {mp.mp.dps} decimal places\")\n",
    "print(f\"ðŸ“Š Notebook ready for validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Functions Definition\n",
    "\n",
    "We define three different test functions to validate the explicit formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function definitions\n",
    "def f1(u, a=3.0, sigma=1.0): \n",
    "    \"\"\"Truncated Gaussian f1\"\"\"\n",
    "    return mp.exp(-u**2/2) if abs(u) <= a else mp.mpf(0)\n",
    "\n",
    "def f2(u, a=2.0): \n",
    "    \"\"\"Truncated Gaussian f2 with steeper decay\"\"\"\n",
    "    return mp.exp(-u**2) if abs(u) <= a else mp.mpf(0)\n",
    "\n",
    "def f3(u, a=2.0): \n",
    "    \"\"\"Polynomial test function f3\"\"\"\n",
    "    return (1 - u**2/4)**2 if abs(u) <= a else mp.mpf(0)\n",
    "\n",
    "# Mellin transform helper\n",
    "def fhat(f, s, lim):\n",
    "    \"\"\"Numerical Mellin transform\"\"\"\n",
    "    return mp.quad(lambda u: f(u) * mp.exp(s * u), [-lim, lim], maxdegree=10)\n",
    "\n",
    "print(\"âœ… Test functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Validation Functions\n",
    "\n",
    "Implementation of the different sides of the explicit formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prime_sum_v1(f, P=1000, K=50):\n",
    "    \"\"\"Prime side calculation - Version 1 (Original)\"\"\"\n",
    "    s = mp.mpf(0)\n",
    "    import sympy as sp\n",
    "    primes = list(sp.primerange(2, 7919))[:P]\n",
    "    for p in primes:\n",
    "        lp = mp.log(p)\n",
    "        for k in range(1, K+1):\n",
    "            s += lp * f(k * lp)\n",
    "    return s\n",
    "\n",
    "def prime_sum_v2(f, P=1000, K=50):\n",
    "    \"\"\"Prime side calculation - Version 2 (Optimized)\"\"\"\n",
    "    s = mp.mpf(0)\n",
    "    # Use mpmath's prime generation for consistency\n",
    "    for i in range(1, P+1):\n",
    "        try:\n",
    "            p = mp.prime(i)\n",
    "            lp = mp.log(p)\n",
    "            for k in range(1, K+1):\n",
    "                s += lp * f(k * lp)\n",
    "        except:\n",
    "            break\n",
    "    return s\n",
    "\n",
    "def A_infty_v1(f, sigma0=2.0, lim=3, T=50):\n",
    "    \"\"\"Archimedean contribution - Version 1 (Original)\"\"\"\n",
    "    def integrand(t):\n",
    "        s = mp.mpc(sigma0, t)\n",
    "        return (mp.digamma(s/2) - mp.log(mp.pi)) * fhat(f, s, lim)\n",
    "    integ = mp.quad(integrand, [-T, T], maxdegree=10) / (2 * mp.pi)\n",
    "    res1 = fhat(f, mp.mpf(1), lim) / mp.mpf(1)\n",
    "    return integ - res1\n",
    "\n",
    "def A_infty_v2(f, sigma0=2.0, lim=3, T=100):\n",
    "    \"\"\"Archimedean contribution - Version 2 (Higher precision)\"\"\"\n",
    "    def integrand(t):\n",
    "        s = mp.mpc(sigma0, t)\n",
    "        kernel = mp.digamma(s/2) - mp.log(mp.pi)\n",
    "        transform = fhat(f, s, lim)\n",
    "        return kernel * transform\n",
    "    \n",
    "    # Higher precision integration\n",
    "    with mp.workdps(mp.mp.dps + 10):\n",
    "        integ = mp.quad(integrand, [-T, T], maxdegree=20, error=True)\n",
    "        res1 = fhat(f, mp.mpf(1), lim) / mp.mpf(1)\n",
    "        return (integ[0] / (2j * mp.pi) - res1).real\n",
    "\n",
    "def zero_sum_v1(f, N=100, lim=3):\n",
    "    \"\"\"Zero sum - Version 1 (Basic)\"\"\"\n",
    "    s = mp.mpf(0)\n",
    "    for n in range(1, N+1):\n",
    "        rho = mp.zetazero(n)\n",
    "        s += fhat(f, mp.im(rho) * 1j, lim).real\n",
    "    return s\n",
    "\n",
    "def zero_sum_v2(f, N=100, lim=3):\n",
    "    \"\"\"Zero sum - Version 2 (Optimized with progress tracking)\"\"\"\n",
    "    s = mp.mpf(0)\n",
    "    zeros_used = []\n",
    "    \n",
    "    for n in range(1, N+1):\n",
    "        rho = mp.zetazero(n)\n",
    "        gamma = mp.im(rho)\n",
    "        zeros_used.append(float(gamma))\n",
    "        \n",
    "        # Use purely imaginary argument for the transform\n",
    "        contrib = fhat(f, 1j * gamma, lim).real\n",
    "        s += contrib\n",
    "        \n",
    "    return s, zeros_used\n",
    "\n",
    "print(\"âœ… Validation functions defined (v1 and v2 versions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Comparison Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comparative_validation(functions_dict, N_zeros_list=[50, 100, 200], versions=['v1', 'v2']):\n",
    "    \"\"\"Run comprehensive validation comparing different versions and parameters\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for func_name, (func, lim) in functions_dict.items():\n",
    "        print(f\"\\nðŸ§ª Testing function: {func_name}\")\n",
    "        \n",
    "        for N in N_zeros_list:\n",
    "            print(f\"  ðŸ“Š N_zeros = {N}\")\n",
    "            \n",
    "            for version in versions:\n",
    "                try:\n",
    "                    print(f\"    ðŸ”„ Version {version}...\")\n",
    "                    \n",
    "                    # Select version functions\n",
    "                    if version == 'v1':\n",
    "                        prime_fn = prime_sum_v1\n",
    "                        arch_fn = A_infty_v1\n",
    "                        zero_fn = zero_sum_v1\n",
    "                    else:\n",
    "                        prime_fn = prime_sum_v2\n",
    "                        arch_fn = A_infty_v2\n",
    "                        zero_fn = zero_sum_v2\n",
    "                    \n",
    "                    # Calculate components\n",
    "                    prime_sum = prime_fn(func, P=min(500, N), K=20)\n",
    "                    arch_sum = arch_fn(func, lim=lim, T=min(50, N//2))\n",
    "                    \n",
    "                    if version == 'v2':\n",
    "                        zero_sum, zeros_list = zero_fn(func, N=N, lim=lim)\n",
    "                    else:\n",
    "                        zero_sum = zero_fn(func, N=N, lim=lim)\n",
    "                        zeros_list = None\n",
    "                    \n",
    "                    # Total arithmetic side\n",
    "                    arithmetic_total = prime_sum + arch_sum\n",
    "                    \n",
    "                    # Calculate errors\n",
    "                    error_abs = abs(arithmetic_total - zero_sum)\n",
    "                    error_rel = error_abs / abs(arithmetic_total) if arithmetic_total != 0 else mp.mpf('inf')\n",
    "                    \n",
    "                    result = {\n",
    "                        'function': func_name,\n",
    "                        'version': version,\n",
    "                        'N_zeros': N,\n",
    "                        'prime_sum': float(prime_sum),\n",
    "                        'arch_sum': float(arch_sum), \n",
    "                        'arithmetic_total': float(arithmetic_total),\n",
    "                        'zero_sum': float(zero_sum),\n",
    "                        'error_abs': float(error_abs),\n",
    "                        'error_rel': float(error_rel),\n",
    "                        'zeros_used': zeros_list\n",
    "                    }\n",
    "                    \n",
    "                    results.append(result)\n",
    "                    \n",
    "                    print(f\"      âœ… Error: {error_abs:.2e} (rel: {error_rel:.2e})\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"      âŒ Error in {version}: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Define test functions\n",
    "test_functions = {\n",
    "    'f1': (f1, 3.0),\n",
    "    'f2': (f2, 2.0),\n",
    "    'f3': (f3, 2.0)\n",
    "}\n",
    "\n",
    "print(\"âœ… Comparative validation framework ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Validation and Generate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the comparative validation\n",
    "print(\"ðŸš€ Starting comprehensive validation...\")\n",
    "\n",
    "validation_results = run_comparative_validation(\n",
    "    test_functions, \n",
    "    N_zeros_list=[25, 50, 100],  # Reduced for demo\n",
    "    versions=['v1', 'v2']\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š Validation completed!\")\n",
    "print(f\"Total results: {len(validation_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_df = validation_results.pivot_table(\n",
    "    index=['function', 'N_zeros'],\n",
    "    columns='version', \n",
    "    values=['error_abs', 'error_rel'],\n",
    "    aggfunc='mean'\n",
    ").round(8)\n",
    "\n",
    "print(\"ðŸ“‹ Error Summary by Function and Version:\")\n",
    "display(HTML(summary_df.to_html(classes='table table-striped')))\n",
    "\n",
    "# Detailed results table\n",
    "print(\"\\nðŸ“ˆ Detailed Results:\")\n",
    "display_cols = ['function', 'version', 'N_zeros', 'arithmetic_total', 'zero_sum', 'error_abs', 'error_rel']\n",
    "display(HTML(validation_results[display_cols].to_html(classes='table table-striped', index=False)))"
   ]
  },
  {
   "cell_type": "markdown", 
   "metadata": {},
   "source": [
    "## Convergence Analysis Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence analysis\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Error vs N_zeros plot\n",
    "plt.subplot(2, 2, 1)\n",
    "for func in validation_results['function'].unique():\n",
    "    for version in validation_results['version'].unique():\n",
    "        data = validation_results[\n",
    "            (validation_results['function'] == func) & \n",
    "            (validation_results['version'] == version)\n",
    "        ]\n",
    "        plt.semilogy(data['N_zeros'], data['error_abs'], \n",
    "                    marker='o', label=f'{func}_{version}', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Number of Zeros')\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.title('Convergence: Error vs Number of Zeros')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Version comparison\n",
    "plt.subplot(2, 2, 2)\n",
    "for func in validation_results['function'].unique():\n",
    "    v1_data = validation_results[\n",
    "        (validation_results['function'] == func) & \n",
    "        (validation_results['version'] == 'v1')\n",
    "    ]\n",
    "    v2_data = validation_results[\n",
    "        (validation_results['function'] == func) & \n",
    "        (validation_results['version'] == 'v2')\n",
    "    ]\n",
    "    \n",
    "    if not v1_data.empty and not v2_data.empty:\n",
    "        plt.semilogy(v1_data['N_zeros'], v1_data['error_abs'], \n",
    "                    'o--', label=f'{func}_v1', alpha=0.7)\n",
    "        plt.semilogy(v2_data['N_zeros'], v2_data['error_abs'], \n",
    "                    's-', label=f'{func}_v2', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Number of Zeros')\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.title('Version Comparison (v1 vs v2)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Relative error heatmap\n",
    "plt.subplot(2, 2, 3)\n",
    "pivot_rel = validation_results.pivot_table(\n",
    "    index='function', columns='N_zeros', values='error_rel', aggfunc='mean'\n",
    ")\n",
    "import seaborn as sns\n",
    "sns.heatmap(pivot_rel, annot=True, fmt='.2e', cmap='YlOrRd')\n",
    "plt.title('Relative Error Heatmap')\n",
    "\n",
    "# Component contribution\n",
    "plt.subplot(2, 2, 4)\n",
    "sample_result = validation_results.iloc[0]\n",
    "components = ['prime_sum', 'arch_sum', 'zero_sum']\n",
    "values = [abs(sample_result[comp]) for comp in components]\n",
    "plt.bar(components, values, alpha=0.7)\n",
    "plt.yscale('log')\n",
    "plt.title(f'Component Magnitudes ({sample_result[\"function\"]})')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('docs/validation_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Convergence plots generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('docs', exist_ok=True)\n",
    "\n",
    "# Export CSV\n",
    "validation_results.to_csv('data/validation_results_detailed.csv', index=False)\n",
    "\n",
    "# Export summary\n",
    "summary_df.to_csv('data/validation_summary.csv')\n",
    "\n",
    "# Create HTML report\n",
    "html_report = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Riemann Hypothesis Validation Report</title>\n",
    "    <style>\n",
    "        body {{ font-family: Arial, sans-serif; margin: 40px; }}\n",
    "        table {{ border-collapse: collapse; width: 100%; }}\n",
    "        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
    "        th {{ background-color: #f2f2f2; }}\n",
    "        .pass {{ color: green; font-weight: bold; }}\n",
    "        .fail {{ color: red; font-weight: bold; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>ðŸ§® Riemann Hypothesis Numerical Validation Report</h1>\n",
    "    <p><strong>Generated:</strong> {pd.Timestamp.now()}</p>\n",
    "    <p><strong>Total tests:</strong> {len(validation_results)}</p>\n",
    "    \n",
    "    <h2>ðŸ“Š Summary Results</h2>\n",
    "    {summary_df.to_html(classes='table table-striped')}\n",
    "    \n",
    "    <h2>ðŸ“ˆ Detailed Results</h2>\n",
    "    {validation_results[display_cols].to_html(classes='table table-striped', index=False)}\n",
    "    \n",
    "    <h2>ðŸŽ¯ Test Status</h2>\n",
    "    <ul>\n",
    "\"\"\"\n",
    "\n",
    "# Add test status\n",
    "for _, row in validation_results.iterrows():\n",
    "    status = \"âœ… PASS\" if row['error_abs'] < 1e-3 else \"âŒ FAIL\"\n",
    "    html_report += f\"<li>{row['function']} {row['version']} (N={row['N_zeros']}): {status}</li>\\n\"\n",
    "\n",
    "html_report += \"\"\"\n",
    "    </ul>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "with open('docs/validation.html', 'w') as f:\n",
    "    f.write(html_report)\n",
    "\n",
    "print(\"âœ… Results exported:\")\n",
    "print(\"  - data/validation_results_detailed.csv\")\n",
    "print(\"  - data/validation_summary.csv\")\n",
    "print(\"  - docs/validation.html\")\n",
    "print(\"  - docs/validation_plots.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
